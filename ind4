import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin
from datetime import datetime, timedelta
import time

def extract_act_list_from_indiacode(base_url, headers, year, start_date, end_date):
    """
    Extracts a list of Acts for a given year, filtered by actual enactment date
    """
    full_url = (
        f"{base_url}"
        f"?page-token=032a83544a3b&page-token-value=98ed972cd138f7f10f4cc9e781e1e030"
        f"&nccharset=8B2E7B36&location=%2F&query=&rpp=100"
        f"&sort_by=dc.date.issued_dt&order=desc"
        f"&filter_field_1=enactmentdate&filter_type_1=contains"
        f"&filter_value_1={year}"
    )

    print(f"üîç Fetching data for year {year} from:\n{full_url}\n")
    response = requests.get(full_url, headers=headers)

    if response.status_code != 200:
        print("‚ùå Failed to fetch the page. Status code:", response.status_code)
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    rows = soup.select("tr")
    results = []

    print(f"üìÑ Found {len(rows)} rows to scan...\n")

    for idx, row in enumerate(rows, 1):
        try:
            title_cell = row.select_one('td[headers="t3"]')
            date_cell = row.select_one('td[headers="t1"]')
            link_cell = row.select_one('td[headers="t4"] a')

            if not title_cell or not date_cell or not link_cell:
                continue

            title = title_cell.get_text(strip=True)
            date_str = date_cell.get_text(strip=True)

            # Convert date string like "30-Apr-2025" to datetime object
            try:
                date_obj = datetime.strptime(date_str, "%d-%b-%Y")
            except ValueError:
                continue  # Skip if date format is invalid

            # Skip articles not in the required range
            if not (start_date <= date_obj <= end_date):
                continue

            relative_link = link_cell.get("href", "").strip()
            full_url = urljoin("https://www.indiacode.nic.in", relative_link)

            print(f"‚ñ∂Ô∏è {idx}. Title: {title}")
            print(f"    üìÖ Date: {date_str}")
            print(f"    üîó URL: {full_url}")

            results.append({
                "Title": title,
                "Date": date_str,
                "Details Page URL": full_url
            })

            time.sleep(0.5)

        except Exception as e:
            print(f"‚ö†Ô∏è Error processing row {idx}: {e}")
            continue

    return results


def main_indiacode_scraper_with_days():
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    base_url = "https://www.indiacode.nic.in/handle/123456789/1362/simple-search"

    try:
        num_days = int(input("üìÜ Enter number of days to look back from today: ").strip())
    except ValueError:
        print("‚ùå Please enter a valid number.")
        return

    end_date = datetime.today()
    start_date = end_date - timedelta(days=num_days)

    print(f"\nüîé Looking for Acts between {start_date.strftime('%d-%b-%Y')} and {end_date.strftime('%d-%b-%Y')}\n")

    # Handle cases where date range spans multiple years
    years = list(range(start_date.year, end_date.year + 1))

    all_data = []
    for year in years:
        yearly_data = extract_act_list_from_indiacode(base_url, headers, year, start_date, end_date)
        all_data.extend(yearly_data)

    if not all_data:
        print("‚ùå No data extracted within the given date range.")
        return

    # Save to Excel
    df = pd.DataFrame(all_data)
    df.to_excel("indiacode_filtered_acts.xlsx", index=False)

    print("\n‚úÖ Done. Data saved to 'indiacode_filtered_acts.xlsx'")
    pd.set_option("display.max_colwidth", None)
    print("\nüìÑ Sample Preview:")
    print(df.head())


# Run it
main_indiacode_scraper_with_days()
