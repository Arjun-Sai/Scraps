import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd
import time
import io
import math
import PyPDF2

def extract_pdf_content(pdf_url, headers):
    try:
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30)
        if pdf_response.status_code != 200:
            return None
        pdf_file = io.BytesIO(pdf_response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text_content = []
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text_content.append(page_text)
        return " ".join(" ".join(text_content).split())
    except Exception:
        return None

def extract_content_with_fallback(base_pdf_url, headers):
    print(f"Trying base PDF first...")
    base_content = extract_pdf_content(base_pdf_url, headers)

    if base_content:
        print(f"Base PDF extracted successfully.")
        return base_content

    print(f"Base failed. Trying multivolume PDFs...")
    content_parts = []
    volume = 1

    while True:
        volume_url = f"{base_pdf_url}/{volume}"
        try:
            response = requests.get(volume_url, headers=headers, timeout=15, stream=True)
            if response.status_code != 200:
                print(f"Volume {volume} not found, stopping.")
                break
            response.close()

            print(f"Found Volume {volume}, extracting...")
            vol_text = extract_pdf_content(volume_url, headers)
            if vol_text:
                content_parts.append(f"=== VOLUME {volume} ===\n\n{vol_text}")
                print(f"Volume {volume} added.")
            else:
                print(f"Volume {volume} extraction failed.")
        except requests.exceptions.RequestException as e:
            print(f"Error with Volume {volume}: {e}")
            break

        volume += 1
        time.sleep(0.5)

    if content_parts:
        return "\n\n" + ("\n\n" + "="*60 + "\n\n").join(content_parts)
    else:
        return "No content extracted"

def scrape_acts(days):
    today = datetime.today().date()
    start_date = (today - timedelta(days=days)).isoformat()
    end_date = today.isoformat()
    headers = {"User-Agent": "Mozilla/5.0"}

    # Step 1: Get total count from API
    count_url = (
        f"https://api.prod.legislation.gov.au/v1/titles/"
        f"search(criteria='registrationdate({start_date},{end_date})')?$count=true"
    )
    response = requests.get(count_url, headers=headers)
    total_articles = response.json().get("@odata.count", 0)
    print(f"\nüìä Found {total_articles} legislation items between {start_date} and {end_date}.")

    results = []
    pages = math.ceil(total_articles / 100)

    for page in range(pages):
        skip = page * 100
        api_url = (
            f"https://api.prod.legislation.gov.au/v1/titles/"
            f"search(criteria='registrationdate({start_date},{end_date})')"
            f"?$select=id,name&$orderby=searchcontexts/fulltextversion/registeredat desc"
            f"&$top=100&$skip={skip}"
        )
        print(f"\nüìÑ Fetching page {page+1}/{pages} from API...")
        response = requests.get(api_url, headers=headers)
        items = response.json().get("value", [])
        print(f"‚û°Ô∏è  Got {len(items)} items.")

        for idx, item in enumerate(items, 1):
            leg_id = item.get("id")
            title = item.get("name")
            full_url = f"https://www.legislation.gov.au/Details/{leg_id}"
            print(f"üîç [{page*100+idx}/{total_articles}] {title}")

            reg_date = "N/A"
            effective_date = "N/A"
            content = "N/A"

            try:
                detail_response = requests.get(full_url, headers=headers, timeout=10)
                if detail_response.status_code == 200:
                    detail_soup = BeautifulSoup(detail_response.text, "html.parser")
                    eff_span = detail_soup.select_one("span.date-effective-start")
                    reg_span = detail_soup.find("div", string=lambda t: t and "Registered:" in t)

                    if reg_span:
                        reg_date = reg_span.get_text(strip=True).split("Registered:")[-1].strip()
                    if eff_span:
                        effective_date = eff_span.text.strip()
                        try:
                            eff_dt_obj = datetime.strptime(effective_date, "%d %B %Y")
                            eff_dt_str = eff_dt_obj.strftime("%Y-%m-%d")
                            pdf_url = f"https://www.legislation.gov.au/{leg_id}/{eff_dt_str}/{eff_dt_str}/text/original/pdf"
                            print(f"üìÑ Attempting to extract content from: {pdf_url}")
                            content = extract_content_with_fallback(pdf_url, headers)
                            if content and len(content) > 10000:
                                content = content[:10000] + "... [TRUNCATED]"
                        except Exception as e:
                            print(f"‚ö†Ô∏è Failed parsing effective date: {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to fetch detail page: {e}")
            time.sleep(1)

            results.append({
                "Title": title,
                "Registered Date": reg_date,
                "URL": full_url,
                "Content": content
            })

        time.sleep(2)

    df = pd.DataFrame(results)
    filename = f"australian_legislation_{days}days_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
    df.to_excel(filename, index=False)
    print(f"\n‚úÖ Done. Saved {len(results)} legislation items to '{filename}'")
    return df

# Entry point
if __name__ == "__main__":
    try:
        user_input = input("üìÖ Enter number of days to look back from today: ").strip()
        days = int(user_input)
        if days <= 0:
            raise ValueError
        scrape_acts(days)
    except ValueError:
        print("‚ùå Please enter a valid positive number.")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Scraping interrupted by user.")
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")
