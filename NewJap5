import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import re
from googletrans import Translator
import time

class PMDAScraper:
    def __init__(self):
        self.base_url = "https://www.pmda.go.jp/0017.html"
        self.translator = Translator()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def parse_japanese_date(self, date_str):
        match = re.search(r'(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•', date_str)
        if match:
            year, month, day = match.groups()
            return datetime(int(year), int(month), int(day))
        return None

    def translate_text(self, text, max_retries=3):
        if not text or text.strip() == "":
            return ""
        for attempt in range(max_retries):
            try:
                if len(text) > 4000:
                    chunks = [text[i:i+4000] for i in range(0, len(text), 4000)]
                    translated = [self.translator.translate(chunk, src='ja', dest='en').text for chunk in chunks]
                    return " ".join(translated)
                else:
                    return self.translator.translate(text, src='ja', dest='en').text
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"Translation failed: {str(e)}")
                    return text

    def scrape_articles(self, days_back=None):
        try:
            print("Fetching webpage...")
            response = self.session.get(self.base_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # Dump page for debugging
            with open("pmda_debug.html", "w", encoding="utf-8") as f:
                f.write(soup.prettify())
            print("Saved HTML to pmda_debug.html ‚Äî inspect this if results are empty.")

            articles = []

            # Robust <ul> finder for any variant like list_news, news_list, list news
            list_news = soup.find('ul', class_=re.compile(r'(list.*news|news.*list)', re.I))
            if not list_news:
                print("‚ùå No news list found. Check pmda_debug.html to confirm the structure.")
                return []

            article_items = list_news.find_all('li')
            print(f"‚úÖ Found {len(article_items)} <li> items under news list.")

            cutoff_date = datetime.now() - timedelta(days=days_back) if days_back else None

            for item in article_items:
                try:
                    date_elem = item.find('p', class_='date')
                    date_str = date_elem.get_text(strip=True) if date_elem else ""
                    article_date = self.parse_japanese_date(date_str)
                    if not article_date:
                        continue
                    if cutoff_date and article_date < cutoff_date:
                        continue

                    title_elem = item.find('p', class_='title')
                    title_ja = title_elem.get_text(strip=True) if title_elem else ""
                    if not title_ja or len(title_ja) < 3:
                        continue

                    link_elem = item.find('a', href=True)
                    if link_elem:
                        href = link_elem['href']
                        if href.startswith('http'):
                            url = href
                        elif href.startswith('/'):
                            url = f"https://www.pmda.go.jp{href}"
                        else:
                            url = f"https://www.pmda.go.jp/0017/html/{href}"
                    else:
                        url = ""

                    category_elem = item.find('p', class_=re.compile(r'category', re.I))
                    category_ja = category_elem.get_text(strip=True) if category_elem else ""

                    content_ja = title_ja  # Placeholder ‚Äî full article scraping not done here

                    print(f"‚úÖ {title_ja} | {article_date.date()} | {category_ja} | {url}")

                    articles.append({
                        'title_ja': title_ja,
                        'date': article_date,
                        'url': url,
                        'content_ja': content_ja,
                        'category_ja': category_ja
                    })

                except Exception as e:
                    print(f"Error processing item: {str(e)}")
                    continue

            print(f"‚úÖ Extracted {len(articles)} articles before deduplication.")
            return articles

        except Exception as e:
            print(f"Error fetching webpage: {str(e)}")
            return []

    def process_articles(self, articles, translate=True):
        processed = []
        for idx, article in enumerate(articles, 1):
            print(f"Processing article {idx}/{len(articles)}: {article['title_ja'][:50]}...")
            try:
                if translate:
                    title_en = self.translate_text(article['title_ja'])
                    content_en = self.translate_text(article['content_ja'])
                    category_en = self.translate_text(article['category_ja']) if article['category_ja'] else ""
                else:
                    title_en = article['title_ja']
                    content_en = article['content_ja']
                    category_en = article['category_ja']

                processed.append({
                    'Article Title': title_en,
                    'Published Date': article['date'].strftime('%Y-%m-%d'),
                    'URL': article['url'],
                    'Content': content_en,
                    'Category': category_en
                })

                if translate:
                    time.sleep(0.5)

            except Exception as e:
                print(f"Error processing: {str(e)}")
                processed.append({
                    'Article Title': article['title_ja'],
                    'Published Date': article['date'].strftime('%Y-%m-%d'),
                    'URL': article['url'],
                    'Content': article['content_ja'],
                    'Category': article['category_ja']
                })
        return processed

    def save_to_excel(self, df, filename=None):
        filename = filename or f"pmda_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        try:
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"‚úÖ Saved to {filename}")
            return filename
        except Exception as e:
            print(f"Error saving Excel: {str(e)}")
            return None

    def run_scraper(self, days_back=None, translate=True, save_excel=True):
        print("\nüöÄ Starting PMDA scraper...")
        articles = self.scrape_articles(days_back)
        if not articles:
            print("No articles found.")
            return None

        processed = self.process_articles(articles, translate)
        df = pd.DataFrame(processed)

        # Remove duplicates
        before = len(df)
        df.drop_duplicates(subset=['Article Title', 'Published Date', 'URL'], inplace=True)
        after = len(df)
        print(f"‚úÖ Removed {before - after} duplicate articles.")

        # Sort by newest first
        df['Published Date'] = pd.to_datetime(df['Published Date'])
        df = df.sort_values('Published Date', ascending=False)
        df['Published Date'] = df['Published Date'].dt.strftime('%Y-%m-%d')

        print(f"‚úÖ Final count: {len(df)} articles after cleanup.")
        if save_excel:
            self.save_to_excel(df)
        return df

def main():
    scraper = PMDAScraper()
    print("PMDA Article Scraper\n===================")

    # Ask user
    days_back = None
    while True:
        days_input = input("Days to look back (Enter for all): ").strip()
        if days_input == "":
            break
        try:
            days_back = int(days_input)
            if days_back < 0:
                print("Please enter a positive number.")
                continue
            break
        except ValueError:
            print("Please enter a valid number.")

    translate = input("Translate to English? (y/n, default y): ").strip().lower() in ['', 'y', 'yes']
    save_excel = input("Save to Excel? (y/n, default y): ").strip().lower() in ['', 'y', 'yes']

    df = scraper.run_scraper(days_back, translate, save_excel)
    if df is not None:
        print("\nFirst few rows:")
        print(df.head())
        print(f"\n‚úÖ Total rows: {len(df)}")

if __name__ == "__main__":
    main()
