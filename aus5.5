import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd
import time
import io
import math
import PyPDF2

def extract_pdf_content(pdf_url, headers):
    try:
        pdf_response = requests.get(pdf_url, headers=headers, timeout=30)
        if pdf_response.status_code != 200:
            return None
        pdf_file = io.BytesIO(pdf_response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text_content = []
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text_content.append(page_text)
        return " ".join(" ".join(text_content).split())
    except Exception:
        return None

def extract_content_with_fallback(base_pdf_url, headers):
    print(f"Trying base PDF first...")
    base_content = extract_pdf_content(base_pdf_url, headers)

    if base_content:
        print(f"Base PDF extracted successfully.")
        return base_content

    print(f"Base failed. Trying multivolume PDFs...")
    content_parts = []
    volume = 1

    while True:
        volume_url = f"{base_pdf_url}/{volume}"
        try:
            response = requests.get(volume_url, headers=headers, timeout=15, stream=True)
            if response.status_code != 200:
                print(f"Volume {volume} not found, stopping.")
                break
            response.close()

            print(f"Found Volume {volume}, extracting...")
            vol_text = extract_pdf_content(volume_url, headers)
            if vol_text:
                content_parts.append(f"=== VOLUME {volume} ===\n\n{vol_text}")
                print(f"Volume {volume} added.")
            else:
                print(f"Volume {volume} extraction failed.")
        except requests.exceptions.RequestException as e:
            print(f"Error with Volume {volume}: {e}")
            break

        volume += 1
        time.sleep(0.5)

    if content_parts:
        return "\n\n" + ("\n\n" + "="*60 + "\n\n").join(content_parts)
    else:
        return "No content extracted"

def get_total_count_from_api(days):
    # Calculate actual dates for the API (it needs ISO format, not relative dates)
    today = datetime.today().date()
    start_date = (today - timedelta(days=days)).isoformat()
    end_date = today.isoformat()
    
    # FIXED: Removed collection(act) filter to get ALL legislation types
    api_url = (
        f"https://api.prod.legislation.gov.au/v1/titles/search(criteria='registrationdate({start_date},{end_date})')"
        f"?$count=true"
    )
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        print(f"üîç API Count URL: {api_url}")
        response = requests.get(api_url, headers=headers)
        if response.status_code == 200:
            count = response.json().get('@odata.count', 0)
            print(f"üìä API returned count: {count}")
            return count
        else:
            print(f"‚ùå API responded with status: {response.status_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è API count failed: {e}")
    return 0

def scrape_acts(days):
    # Get accurate count using the proper API for ALL legislation types
    total_articles = get_total_count_from_api(days)
    
    # IMPROVED: Better handling of pagination
    if total_articles == 0:
        print("‚ö†Ô∏è No legislation found for the specified date range or API failed.")
        print("Proceeding with dynamic scraping...")
        total_pages = 5  # Reduced fallback
    else:
        total_pages = math.ceil(total_articles / 100)
    
    print(f"üîç Total legislation found: {total_articles} | Pages to scrape: {total_pages}\n")

    headers = {"User-Agent": "Mozilla/5.0"}
    results = []

    for page in range(total_pages):
        skip = page * 100
        # FIXED: Removed /collection(act) to get ALL legislation types
        url = (
            f"https://www.legislation.gov.au/search/registrationdate(today-{days},today)"
            f"/sort(registeredat%20desc)?skip={skip}"
        )

        print(f"\nüìÑ Scraping page {page + 1}: {url}\n")
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"‚ùå Failed to fetch page {page+1}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        items = soup.select("frl-grid-cell-title-name-in-force .title-name a")
        
        # IMPROVED: Break if no items found (end of results)
        if not items:
            print(f"‚úÖ No more items found on page {page + 1}. Stopping.")
            break

        print(f"üìã Found {len(items)} items on page {page + 1}")

        for idx, item in enumerate(items, 1):
            current_item = skip + idx
            print(f"‚Üí Processing item {current_item}/{total_articles}: {item.text.strip()[:50]}...")
            
            title = item.text.strip()
            relative_link = item.get("href", "").strip()
            base_url = "https://www.legislation.gov.au"
            full_url = base_url + relative_link if relative_link else "N/A"

            # IMPROVED: Better registration date extraction
            reg_info = item.find_parent().find_next_sibling("div")
            reg_date = "N/A"
            if reg_info and "Registered:" in reg_info.text:
                reg_date = reg_info.get_text(strip=True).split("Registered:")[-1].strip()

            effective_date = "N/A"
            content = "N/A"
            download_link = "N/A"

            # Extract legislation ID from URL (works for all types, not just acts)
            leg_id = relative_link.split("/")[1] if relative_link else None
            if full_url != "N/A":
                try:
                    detail_response = requests.get(full_url, headers=headers, timeout=10)
                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.text, "html.parser")
                        eff_span = detail_soup.select_one("span.date-effective-start")
                        if eff_span:
                            effective_date = eff_span.text.strip()
                            try:
                                eff_dt_obj = datetime.strptime(effective_date, "%d %B %Y")
                                eff_dt_str = eff_dt_obj.strftime("%Y-%m-%d")
                                pdf_url = f"{base_url}/{leg_id}/{eff_dt_str}/{eff_dt_str}/text/original/pdf"
                                download_link = pdf_url
                                print(f"üìÑ Extracting PDF content...")
                                content = extract_content_with_fallback(pdf_url, headers)

                                if len(content) > 10000:
                                    content = content[:10000] + "... [TRUNCATED]"
                            except Exception as e:
                                print(f"‚ö†Ô∏è Date parsing failed: {e}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Detail page failed: {e}")
                
                time.sleep(1)

            results.append({
                "Title": title,
                "Registered Date": reg_date,
                "URL": full_url,
                "Content": content
            })

        # IMPROVED: Add delay between pages
        time.sleep(2)

    df = pd.DataFrame(results)
    
    # IMPROVED: Better filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"australian_legislation_{days}days_{timestamp}.xlsx"
    
    df.to_excel(filename, index=False)
    print(f"\n‚úÖ Done. Saved {len(results)} legislation items to '{filename}'")
    print(f"üìä Expected: {total_articles} items | Actually scraped: {len(results)} items")
    print(df[["Title", "Registered Date", "URL"]].head())
    
    return df

# Run with user input
if __name__ == "__main__":
    try:
        user_input = input("üìÖ Enter number of days to look back from today: ").strip()
        days = int(user_input)
        if days <= 0:
            raise ValueError
        scrape_acts(days)  # Function name remains the same for compatibility
    except ValueError:
        print("‚ùå Please enter a valid positive number.")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Scraping interrupted by user.")
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")
